{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3607fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: flappy-bird-gym in c:\\users\\jooni\\appdata\\roaming\\python\\python39\\site-packages (0.3.0)\n",
      "Requirement already satisfied: gym~=0.18.0 in c:\\users\\jooni\\appdata\\roaming\\python\\python39\\site-packages (from flappy-bird-gym) (0.18.3)\n",
      "Requirement already satisfied: numpy~=1.19.5 in c:\\users\\jooni\\appdata\\roaming\\python\\python39\\site-packages (from flappy-bird-gym) (1.19.5)\n",
      "Requirement already satisfied: pygame~=2.0.1 in c:\\users\\jooni\\appdata\\roaming\\python\\python39\\site-packages (from flappy-bird-gym) (2.0.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from gym~=0.18.0->flappy-bird-gym) (1.7.3)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in c:\\users\\jooni\\appdata\\roaming\\python\\python39\\site-packages (from gym~=0.18.0->flappy-bird-gym) (1.5.15)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in c:\\users\\jooni\\appdata\\roaming\\python\\python39\\site-packages (from gym~=0.18.0->flappy-bird-gym) (8.2.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\jooni\\appdata\\roaming\\python\\python39\\site-packages (from gym~=0.18.0->flappy-bird-gym) (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install flappy-bird-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2798ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Run the game with or without visualization\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mrun_flappy_bird\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_visualization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mrun_flappy_bird\u001b[1;34m(with_visualization)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_visualization:\n\u001b[0;32m     19\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Control FPS for visualization\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Implement your learning algorithm here\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Update your model based on the observation, reward, etc.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check if the game is over\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "def run_flappy_bird(with_visualization=False):\n",
    "    # Initialize the environment\n",
    "    env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Replace this with your model's action based on the observation\n",
    "        action = env.action_space.sample()  # Currently taking a random action\n",
    "\n",
    "        # Process the action in the environment\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # Rendering for visualization\n",
    "        if with_visualization:\n",
    "            env.render()\n",
    "            time.sleep(1 / 30)  # Control FPS for visualization\n",
    "\n",
    "        # Implement your learning algorithm here\n",
    "        # Update your model based on the observation, reward, etc.\n",
    "\n",
    "        # Check if the game is over\n",
    "        if done:\n",
    "            obs = env.reset()  # Start a new game\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b703aa",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Neural Network Model for DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        # Define your neural network layers here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        return x\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    # Preprocess the observation (state) here\n",
    "    return obs\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    # Implement epsilon-greedy policy here\n",
    "    pass\n",
    "\n",
    "def optimize_model():\n",
    "    # Implement the optimization step here\n",
    "    pass\n",
    "\n",
    "# Parameters\n",
    "input_size = ... # Define the input size\n",
    "n_actions = ... # Define the number of actions\n",
    "replay_memory_size = 10000\n",
    "batch_size = 128\n",
    "epsilon_start = 0.9\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 200\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Main DQN setup\n",
    "policy_net = DQN(input_size, n_actions)\n",
    "target_net = DQN(input_size, n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=learning_rate)\n",
    "memory = deque([], maxlen=replay_memory_size)\n",
    "steps_done = 0\n",
    "\n",
    "# Main training loop\n",
    "def train_dqn():\n",
    "    env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "    state = preprocess_observation(env.reset())\n",
    "    for t in count():\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * \\\n",
    "            math.exp(-1. * steps_done / epsilon_decay)\n",
    "        action = choose_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = preprocess_observation(next_state)\n",
    "\n",
    "        memory.append((state, action, next_state, reward, done))\n",
    "        state = next_state\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            state = preprocess_observation(env.reset())\n",
    "\n",
    "        if t % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    env.close()\n",
    "\n",
    "train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb034cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Discretize the state space\n",
    "def discretize_state(obs):\n",
    "    # Implement discretization logic\n",
    "    # Return a discrete state\n",
    "    pass\n",
    "\n",
    "# Initialize Q-table\n",
    "num_states = ... # Define based on your discretization\n",
    "num_actions = 2 # flap or not flap\n",
    "Q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Q-learning parameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "# Choose an action\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice([0, 1]) # explore\n",
    "    else:\n",
    "        return np.argmax(Q_table[state]) # exploit\n",
    "\n",
    "# Update Q-table\n",
    "def update_Q_table(state, action, reward, new_state):\n",
    "    best_future_reward = np.max(Q_table[new_state])\n",
    "    Q_table[state, action] = Q_table[state, action] + \\\n",
    "                             learning_rate * (reward + discount_factor * best_future_reward - Q_table[state, action])\n",
    "\n",
    "# Training loop\n",
    "def train_q_learning():\n",
    "    env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "    obs = env.reset()\n",
    "    state = discretize_state(obs)\n",
    "\n",
    "    while True:\n",
    "        action = choose_action(state)\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        new_state = discretize_state(new_obs)\n",
    "\n",
    "        update_Q_table(state, action, reward, new_state)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            state = discretize_state(obs)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "train_q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a51060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the game with or without visualization\n",
    "run_flappy_bird(with_visualization=True)  # Set to False for training without visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
